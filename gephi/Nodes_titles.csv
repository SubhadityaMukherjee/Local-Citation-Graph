Id	Label	Author	PrettyName
deep inside convolutional networks visualising image classification models and saliency maps	Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps	Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew	Simonyan et al. 2014
why should i trust you explaining the predictions of any classifier	"Why Should I Trust You?": Explaining the Predictions of Any Classifier	Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos	Ribeiro et al. 2016
distilling the knowledge in a neural network	Distilling the Knowledge in a Neural Network	Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff	Hinton et al. 2015
striving for simplicity the all convolutional net	Striving for Simplicity: The All Convolutional Net	Springenberg, Jost Tobias; Dosovitskiy, Alexey; Brox, Thomas; Riedmiller, Martin	Springenberg et al. 2015
interpretation of neural networks is fragile	Interpretation of Neural Networks is Fragile	Ghorbani, Amirata; Abid, Abubakar; Zou, James	Ghorbani et al. 2018
the unreliability of saliency methods	The (Un)reliability of saliency methods	Kindermans, Pieter-Jan; Hooker, Sara; Adebayo, Julius; Alber, Maximilian; Schütt, Kristof T.; Dähne, Sven; Erhan, Dumitru; Kim, Been	Kindermans et al. 2017
network in network	Network In Network	Lin, Min; Chen, Qiang; Yan, Shuicheng	Lin et al. 2014
visionlanguage pretraining basics recent advances and future trends	Vision-Language Pre-training: Basics, Recent Advances, and Future Trends	Gan, Zhe; Li, Linjie; Li, Chunyuan; Wang, Lijuan; Liu, Zicheng; Gao, Jianfeng	Gan et al. 2022
how does mixup help with robustness and generalization	How Does Mixup Help With Robustness and Generalization?	Zhang, Linjun; Deng, Zhun; Kawaguchi, Kenji; Ghorbani, Amirata; Zou, James	Zhang et al. 2021
real time image saliency for black box classifiers	Real Time Image Saliency for Black Box Classifiers	Dabkowski, Piotr; Gal, Yarin	Dabkowski &  Gal 2017
network dissection quantifying interpretability of deep visual representations	Network Dissection: Quantifying Interpretability of Deep Visual Representations	Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio	Bau et al. 2017
understanding deep networks via extremal perturbations and smooth masks	Understanding Deep Networks via Extremal Perturbations and Smooth Masks	Fong, Ruth; Patrick, Mandela; Vedaldi, Andrea	Fong et al. 2019
methods for interpreting and understanding deep neural networks	Methods for Interpreting and Understanding Deep Neural Networks	Montavon, Grégoire; Samek, Wojciech; Müller, Klaus-Robert	Montavon et al. 2018
explainable ai beware of inmates running the asylum or how i learnt to stop worrying and love the social and behavioural sciences	Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences	Miller, Tim; Howe, Piers; Sonenberg, Liz	Miller et al. 2017
explaining explanations an overview of interpretability of machine learning	Explaining Explanations: An Overview of Interpretability of Machine Learning	Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana	Gilpin et al. 2019
explainable artificial intelligence xai concepts taxonomies opportunities and challenges toward responsible ai	Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI	Arrieta, Alejandro Barredo; Díaz-Rodríguez, Natalia; Del Ser, Javier; Bennetot, Adrien; Tabik, Siham; Barbado, Alberto; García, Salvador; Gil-López, Sergio; Molina, Daniel; Benjamins, Richard; Chatila, Raja; Herrera, Francisco	Arrieta et al. 2019
explanation in humanai systems a literature metareview synopsis of key ideas and publications and bibliography for explainable ai	Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI	Mueller, Shane T.; Hoffman, Robert R.; Clancey, William; Emrey, Abigail; Klein, Gary	Mueller et al. 2019
explanations can be manipulated and geometry is to blame	Explanations can be manipulated and geometry is to blame	Dombrowski, Ann-Kathrin; Alber, Maximilian; Anders, Christopher J.; Ackermann, Marcel; Müller, Klaus-Robert; Kessel, Pan	Dombrowski et al. 2019
did the model understand the question	Did the Model Understand the Question?	Mudrakarta, Pramod Kaushik; Taly, Ankur; Sundararajan, Mukund; Dhamdhere, Kedar	Mudrakarta et al. 2018
smoothgrad removing noise by adding noise	SmoothGrad: removing noise by adding noise	Smilkov, Daniel; Thorat, Nikhil; Kim, Been; Viégas, Fernanda; Wattenberg, Martin	Smilkov et al. 2017
sanity checks for saliency maps	Sanity Checks for Saliency Maps	Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been	Adebayo et al. 2020
how important is a neuron	How Important Is a Neuron?	Dhamdhere, Kedar; Sundararajan, Mukund; Yan, Qiqi	Dhamdhere et al. 2018
computationally efficient measures of internal neuron importance	Computationally Efficient Measures of Internal Neuron Importance	Shrikumar, Avanti; Su, Jocelin; Kundaje, Anshul	Shrikumar et al. 2018
learning important features through propagating activation differences	Learning Important Features Through Propagating Activation Differences	Shrikumar, Avanti; Greenside, Peyton; Kundaje, Anshul	Shrikumar et al. 2019
a unified approach to interpreting model predictions	A Unified Approach to Interpreting Model Predictions	Lundberg, Scott M; Lee, Su-In	Lundberg &  Lee 2017
influencedirected explanations for deep convolutional networks	Influence-Directed Explanations for Deep Convolutional Networks	Leino, Klas; Sen, Shayak; Datta, Anupam; Fredrikson, Matt; Li, Linyi	Leino et al. 2018
visualizing and understanding convolutional networks	Visualizing and Understanding Convolutional Networks	Zeiler, Matthew D.; Fergus, Rob	Zeiler &  Fergus 2013
on the infidelity and sensitivity for explanations	On the (In)fidelity and Sensitivity for Explanations	Yeh, Chih-Kuan; Hsieh, Cheng-Yu; Suggala, Arun Sai; Inouye, David I.; Ravikumar, Pradeep	Yeh et al. 2019
towards better understanding of gradientbased attribution methods for deep neural networks	Towards better understanding of gradient-based attribution methods for Deep Neural Networks	Ancona, Marco; Ceolini, Enea; Öztireli, Cengiz; Gross, Markus	Ancona et al. 2018
a survey on neural network interpretability	A Survey on Neural Network Interpretability	Zhang, Yu; Tiňo, Peter; Leonardis, Aleš; Tang, Ke	Zhang et al. 2021ASurveyonNeuralNet
randaugment practical automated data augmentation with a reduced search space	Randaugment: Practical automated data augmentation with a reduced search space	Cubuk, Ekin D.; Zoph, Barret; Shlens, Jonathon; Le, Quoc V.	Cubuk et al. 2020
how to train your vit data augmentation and regularization in vision transformers	How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers	Steiner, Andreas; Kolesnikov, Alexander; Zhai, Xiaohua; Wightman, Ross; Uszkoreit, Jakob; Beyer, Lucas	Steiner et al. 2022
deflating dataset bias using synthetic data augmentation	Deflating Dataset Bias Using Synthetic Data Augmentation	Jaipuria, Nikita; Zhang, Xianling; Bhasin, Rohan; Arafa, Mayar; Chakravarty, Punarjay; Shrivastava, Shubham; Manglani, Sagar; Murali, Vidya N.	Jaipuria et al. 2020
data augmentation can improve robustness	Data Augmentation Can Improve Robustness	Rebufﬁ, Sylvestre-Alvise; Gowal, Sven; Calian, Dan; Stimberg, Florian; Wiles, Olivia; Mann, Timothy	Rebufﬁ et al. 
fixing data augmentation to improve adversarial robustness	Fixing Data Augmentation to Improve Adversarial Robustness	Rebuffi, Sylvestre-Alvise; Gowal, Sven; Calian, Dan A.; Stimberg, Florian; Wiles, Olivia; Mann, Timothy	Rebuffi et al. 2021
augmix a simple data processing method to improve robustness and uncertainty	AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty	Hendrycks, Dan; Mu, Norman; Cubuk, Ekin D.; Zoph, Barret; Gilmer, Justin; Lakshminarayanan, Balaji	Hendrycks et al. 2020
explainability for artificial intelligence in healthcare a multidisciplinary perspective	Explainability for artificial intelligence in healthcare: a multidisciplinary perspective	the Precise4Q consortium; Amann, Julia; Blasimme, Alessandro; Vayena, Effy; Frey, Dietmar; Madai, Vince I.	the Precise4Q consortium et al. 2020
building ethics into artificial intelligence	Building Ethics into Artificial Intelligence	Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang	Yu et al. 2018
opportunities and challenges in explainable artificial intelligence xai a survey	Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey	Das, Arun; Rad, Paul	Das &  Rad 2020
putting ai ethics to work are the tools fit for purpose	Putting AI ethics to work: are the tools fit for purpose?	Ayling, Jacqui; Chapman, Adriane	Ayling &  Chapman 2022
the ethics of ai in health care a mapping review	The ethics of AI in health care: A mapping review	Morley, Jessica; Machado, Caio C.V.; Burr, Christopher; Cowls, Josh; Joshi, Indra; Taddeo, Mariarosaria; Floridi, Luciano	Morley et al. 2020
principles alone cannot guarantee ethical ai	Principles alone cannot guarantee ethical AI	Mittelstadt, Brent	Mittelstadt  2019
a survey on ethical principles of ai and implementations	A Survey on Ethical Principles of AI and Implementations	Zhou, Jianlong; Chen, Fang; Berry, Adam; Reed, Mike; Zhang, Shujia; Savage, Siobhan	Zhou et al. 2020
ethical principles and governance technology development of ai in china elsevier enhanced reader	Ethical Principles and Governance Technology Development of AI in China | Elsevier Enhanced Reader		  
the forgotten margins of ai ethics	The Forgotten Margins of AI Ethics	Birhane, Abeba; Ruane, Elayne; Laurent, Thomas; S. Brown, Matthew; Flowers, Johnathan; Ventresque, Anthony; L. Dancy, Christopher	Birhane et al. 2022
coverage of ethics within the artificial intelligence and machine learning academic literature the case of disabled people	Coverage of ethics within the artificial intelligence and machine learning academic literature: The case of disabled people	Lillywhite, Aspen; Wolbring, Gregor	Lillywhite &  Wolbring 2021
unbiased look at dataset bias	Unbiased look at dataset bias	Torralba, Antonio; Efros, Alexei A.	Torralba &  Efros 2011
chatgpt is not all you need a state of the art review of large generative ai models	ChatGPT is not all you need. A State of the Art Review of large Generative AI models	Gozalo-Brizuela, Roberto; Garrido-Merchan, Eduardo C.	Gozalo-Brizuela &  Garrido-Merchan 2023
