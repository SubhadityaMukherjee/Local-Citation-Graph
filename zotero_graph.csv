"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"3GK2FTVS","preprint","2014","Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew","Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","","","","","http://arxiv.org/abs/1312.6034","This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].","2014-04-19","2022-11-18 12:27:34","2022-11-18 12:27:35","2022-11-18 12:27:34","","","","","","","Deep Inside Convolutional Networks","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1312.6034 [cs]","","/Users/eragon/Zotero/storage/EXEY4ASY/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1312.6034","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TSID7Z5N","preprint","2016","Ribeiro, Marco Tulio; Singh, Sameer; Guestrin, Carlos","""Why Should I Trust You?"": Explaining the Predictions of Any Classifier","","","","","http://arxiv.org/abs/1602.04938","Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.","2016-08-09","2022-11-18 12:27:43","2022-11-18 12:27:43","2022-11-18 12:27:43","","","","","","","""Why Should I Trust You?","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1602.04938 [cs, stat]","","/Users/eragon/Zotero/storage/J66379K9/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1602.04938","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C8SLUCVQ","preprint","2015","Hinton, Geoffrey; Vinyals, Oriol; Dean, Jeff","Distilling the Knowledge in a Neural Network","","","","","http://arxiv.org/abs/1503.02531","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.","2015-03-09","2022-11-18 12:27:46","2022-11-18 12:27:46","2022-11-18 12:27:46","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1503.02531 [cs, stat]","","/Users/eragon/Zotero/storage/TRICABTJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1503.02531","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5HSJU6F2","preprint","2015","Springenberg, Jost Tobias; Dosovitskiy, Alexey; Brox, Thomas; Riedmiller, Martin","Striving for Simplicity: The All Convolutional Net","","","","10.48550/arXiv.1412.6806","http://arxiv.org/abs/1412.6806","Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the ""deconvolution approach"" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.","2015-04-13","2022-11-18 12:29:43","2022-11-18 12:29:43","2022-11-18 12:29:43","","","","","","","Striving for Simplicity","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1412.6806 [cs]","","/Users/eragon/Zotero/storage/DQH9A4MX/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1412.6806","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RJLN5TSA","preprint","2018","Ghorbani, Amirata; Abid, Abubakar; Zou, James","Interpretation of Neural Networks is Fragile","","","","10.48550/arXiv.1710.10547","http://arxiv.org/abs/1710.10547","In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.","2018-11-06","2022-11-18 12:37:26","2022-11-18 12:37:26","2022-11-18 12:37:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1710.10547 [cs, stat]","","/Users/eragon/Zotero/storage/9YPQ3RXB/Ghorbani et al. - 2018 - Interpretation of Neural Networks is Fragile.pdf; /Users/eragon/Zotero/storage/K7RINCR7/Ghorbani et al. - 2018 - Interpretation of Neural Networks is Fragile.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1710.10547","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FXYNGHZ4","preprint","2017","Kindermans, Pieter-Jan; Hooker, Sara; Adebayo, Julius; Alber, Maximilian; Schütt, Kristof T.; Dähne, Sven; Erhan, Dumitru; Kim, Been","The (Un)reliability of saliency methods","","","","10.48550/arXiv.1711.00867","http://arxiv.org/abs/1711.00867","Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.","2017-11-02","2022-11-18 12:37:32","2022-11-18 12:37:32","2022-11-18 12:37:32","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1711.00867 [cs, stat]","","/Users/eragon/Zotero/storage/EBUDRT4E/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1711.00867","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DUH6GKKK","preprint","2014","Lin, Min; Chen, Qiang; Yan, Shuicheng","Network In Network","","","","10.48550/arXiv.1312.4400","http://arxiv.org/abs/1312.4400","We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","2014-03-04","2022-11-18 12:38:46","2022-11-18 12:38:46","2022-11-18 12:38:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1312.4400 [cs]","","/Users/eragon/Zotero/storage/ZYBABWJK/Lin et al. - 2014 - Network In Network.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1312.4400","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D2G3N3RS","preprint","2022","Gan, Zhe; Li, Linjie; Li, Chunyuan; Wang, Lijuan; Liu, Zicheng; Gao, Jianfeng","Vision-Language Pre-training: Basics, Recent Advances, and Future Trends","","","","","http://arxiv.org/abs/2210.09263","This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classiﬁcation, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using speciﬁc systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, uniﬁed modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.","2022-10-17","2022-11-19 16:00:03","2022-11-19 16:00:03","2022-11-19 16:00:03","","","","","","","Vision-Language Pre-training","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2210.09263 [cs]","","/Users/eragon/Zotero/storage/RZ5EPBRS/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2210.09263","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"857SPYI9","conferencePaper","2021","Zhang, Linjun; Deng, Zhun; Kawaguchi, Kenji; Ghorbani, Amirata; Zou, James","How Does Mixup Help With Robustness and Generalization?","","","","","https://openreview.net/forum?id=8yKEo06dKNo","Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.","2021-03-17","2022-11-20 12:19:47","2022-11-20 12:19:47","2022-11-20 12:19:47","","","","","","","","","","","","","","en","","","","","openreview.net","","","","/Users/eragon/Zotero/storage/67D6JV6N/Zhang et al. - 2021 - How Does Mixup Help With Robustness and Generaliza.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","International Conference on Learning Representations","","","","","","","","","","","","","","",""
"S4K4WQZ7","preprint","2017","Dabkowski, Piotr; Gal, Yarin","Real Time Image Saliency for Black Box Classifiers","","","","10.48550/arXiv.1705.07857","http://arxiv.org/abs/1705.07857","In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.","2017-05-22","2022-11-24 09:23:47","2022-11-24 09:23:47","2022-11-24 09:23:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1705.07857 [stat]","","/Users/eragon/Zotero/storage/AFR2PAGY/Dabkowski and Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf","","","Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1705.07857","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DID5CGNJ","preprint","2017","Bau, David; Zhou, Bolei; Khosla, Aditya; Oliva, Aude; Torralba, Antonio","Network Dissection: Quantifying Interpretability of Deep Visual Representations","","","","10.48550/arXiv.1704.05796","http://arxiv.org/abs/1704.05796","We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.","2017-04-19","2022-11-24 09:23:53","2022-11-24 09:23:53","2022-11-24 09:23:53","","","","","","","Network Dissection","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1704.05796 [cs]","","/Users/eragon/Zotero/storage/EDW6KRBC/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence; I.2.10","","","","","","","","","","","","","","","","","","","arXiv:1704.05796","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"D3VRTZJV","preprint","2019","Fong, Ruth; Patrick, Mandela; Vedaldi, Andrea","Understanding Deep Networks via Extremal Perturbations and Smooth Masks","","","","10.48550/arXiv.1910.08485","http://arxiv.org/abs/1910.08485","The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.","2019-10-18","2022-11-24 09:24:00","2022-11-24 09:24:00","2022-11-24 09:24:00","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.08485 [cs, stat]","","/Users/eragon/Zotero/storage/FDMYY4EH/Fong et al. - 2019 - Understanding Deep Networks via Extremal Perturbat.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1910.08485","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVBS2UB4","journalArticle","2018","Montavon, Grégoire; Samek, Wojciech; Müller, Klaus-Robert","Methods for Interpreting and Understanding Deep Neural Networks","Digital Signal Processing","","10512004","10.1016/j.dsp.2017.10.011","http://arxiv.org/abs/1706.07979","This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.","2018-02","2022-11-24 09:24:05","2022-11-24 09:24:05","2022-11-24 09:24:05","1-15","","","73","","Digital Signal Processing","","","","","","","","","","","","","arXiv.org","","arXiv:1706.07979 [cs, stat]","","/Users/eragon/Zotero/storage/R96FQEWE/Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VQVXDHR8","preprint","2017","Miller, Tim; Howe, Piers; Sonenberg, Liz","Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences","","","","10.48550/arXiv.1712.00547","http://arxiv.org/abs/1712.00547","In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.","2017-12-04","2022-11-24 09:24:13","2022-11-24 09:24:13","2022-11-24 09:24:13","","","","","","","Explainable AI","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1712.00547 [cs]","","/Users/eragon/Zotero/storage/9W8R7Q8N/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:1712.00547","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"79XM8CGG","preprint","2019","Gilpin, Leilani H.; Bau, David; Yuan, Ben Z.; Bajwa, Ayesha; Specter, Michael; Kagal, Lalana","Explaining Explanations: An Overview of Interpretability of Machine Learning","","","","10.48550/arXiv.1806.00069","http://arxiv.org/abs/1806.00069","There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","2019-02-03","2022-11-24 09:24:19","2022-11-24 09:24:19","2022-11-24 09:24:19","","","","","","","Explaining Explanations","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1806.00069 [cs, stat]","","/Users/eragon/Zotero/storage/MZWDDSBU/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1806.00069","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PW5XU2YW","preprint","2019","Arrieta, Alejandro Barredo; Díaz-Rodríguez, Natalia; Del Ser, Javier; Bennetot, Adrien; Tabik, Siham; Barbado, Alberto; García, Salvador; Gil-López, Sergio; Molina, Daniel; Benjamins, Richard; Chatila, Raja; Herrera, Francisco","Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI","","","","10.48550/arXiv.1910.10045","http://arxiv.org/abs/1910.10045","In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.","2019-12-26","2022-11-24 09:24:22","2022-11-24 09:24:22","2022-11-24 09:24:22","","","","","","","Explainable Artificial Intelligence (XAI)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1910.10045 [cs]","","/Users/eragon/Zotero/storage/24ZS9T3Q/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf; /Users/eragon/Zotero/storage/DPZTMKX5/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1910.10045","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Y7TGP9N6","preprint","2019","Mueller, Shane T.; Hoffman, Robert R.; Clancey, William; Emrey, Abigail; Klein, Gary","Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI","","","","10.48550/arXiv.1902.01876","http://arxiv.org/abs/1902.01876","This is an integrative review that address the question, ""What makes for a good explanation?"" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.","2019-02-05","2022-11-24 09:24:26","2022-11-24 09:24:26","2022-11-24 09:24:26","","","","","","","Explanation in Human-AI Systems","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1902.01876 [cs]","","/Users/eragon/Zotero/storage/6X7WSPJM/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf; /Users/eragon/Zotero/storage/YS246RSU/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:1902.01876","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EH2UY538","preprint","2019","Dombrowski, Ann-Kathrin; Alber, Maximilian; Anders, Christopher J.; Ackermann, Marcel; Müller, Klaus-Robert; Kessel, Pan","Explanations can be manipulated and geometry is to blame","","","","10.48550/arXiv.1906.07983","http://arxiv.org/abs/1906.07983","Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.","2019-09-25","2022-11-24 09:24:31","2022-11-24 09:24:31","2022-11-24 09:24:31","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1906.07983 [cs, stat]","","/Users/eragon/Zotero/storage/9FUQAKVN/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf; /Users/eragon/Zotero/storage/YI5IAMS5/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Cryptography and Security","","","","","","","","","","","","","","","","","","","arXiv:1906.07983","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4AA8BS2H","preprint","2018","Mudrakarta, Pramod Kaushik; Taly, Ankur; Sundararajan, Mukund; Dhamdhere, Kedar","Did the Model Understand the Question?","","","","10.48550/arXiv.1805.05492","http://arxiv.org/abs/1805.05492","We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \emph{attribution} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from $61.1\%$ to $19\%$, and that of a tabular question answering model from $33.5\%$ to $3.3\%$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.","2018-05-14","2022-11-28 10:55:02","2022-11-28 10:55:02","2022-11-28 10:55:02","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1805.05492 [cs]","","/Users/eragon/Zotero/storage/87TE479A/Mudrakarta et al. - 2018 - Did the Model Understand the Question.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1805.05492","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BJFVH27H","preprint","2017","Smilkov, Daniel; Thorat, Nikhil; Kim, Been; Viégas, Fernanda; Wattenberg, Martin","SmoothGrad: removing noise by adding noise","","","","10.48550/arXiv.1706.03825","http://arxiv.org/abs/1706.03825","Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.","2017-06-12","2022-11-28 10:55:07","2022-11-28 10:55:07","2022-11-28 10:55:07","","","","","","","SmoothGrad","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1706.03825 [cs, stat]","","/Users/eragon/Zotero/storage/YCR6ABCK/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1706.03825","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5ASCXEV","preprint","2020","Adebayo, Julius; Gilmer, Justin; Muelly, Michael; Goodfellow, Ian; Hardt, Moritz; Kim, Been","Sanity Checks for Saliency Maps","","","","10.48550/arXiv.1810.03292","http://arxiv.org/abs/1810.03292","Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.","2020-11-06","2022-11-28 10:55:11","2022-11-28 10:55:11","2022-11-28 10:55:11","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.03292 [cs, stat]","","/Users/eragon/Zotero/storage/DJDSS9U5/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1810.03292","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"36PS9CTG","preprint","2018","Dhamdhere, Kedar; Sundararajan, Mukund; Yan, Qiqi","How Important Is a Neuron?","","","","10.48550/arXiv.1805.12233","http://arxiv.org/abs/1805.12233","The problem of attributing a deep network's prediction to its \emph{input/base} features is well-studied. We introduce the notion of \emph{conductance} to extend the notion of attribution to the understanding the importance of \emph{hidden} units. Informally, the conductance of a hidden unit of a deep network is the \emph{flow} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.","2018-05-30","2022-11-28 10:55:14","2022-11-28 10:55:14","2022-11-28 10:55:14","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1805.12233 [cs, stat]","","/Users/eragon/Zotero/storage/RBRVP5KX/Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1805.12233","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CTULG3YW","preprint","2018","Shrikumar, Avanti; Su, Jocelin; Kundaje, Anshul","Computationally Efficient Measures of Internal Neuron Importance","","","","10.48550/arXiv.1807.09946","http://arxiv.org/abs/1807.09946","The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a ""natural refinement of Integrated Gradients"" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients","2018-07-25","2022-11-28 10:55:17","2022-11-28 10:55:17","2022-11-28 10:55:17","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1807.09946 [cs, stat]","","/Users/eragon/Zotero/storage/MZVID3Y2/Shrikumar et al. - 2018 - Computationally Efficient Measures of Internal Neu.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1807.09946","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VBJURMLT","preprint","2019","Shrikumar, Avanti; Greenside, Peyton; Kundaje, Anshul","Learning Important Features Through Propagating Activation Differences","","","","10.48550/arXiv.1704.02685","http://arxiv.org/abs/1704.02685","The purported ""black box"" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.","2019-10-12","2022-11-28 10:55:20","2022-11-28 10:55:20","2022-11-28 10:55:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1704.02685 [cs]","","/Users/eragon/Zotero/storage/CSAZYN2B/Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1704.02685","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2F5V86ZN","conferencePaper","2017","Lundberg, Scott M; Lee, Su-In","A Unified Approach to Interpreting Model Predictions","Advances in Neural Information Processing Systems","","","","https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html","Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.","2017","2022-11-28 10:55:42","2022-11-28 10:55:42","2022-11-28 10:55:42","","","","30","","","","","","","","Curran Associates, Inc.","","","","","","","Neural Information Processing Systems","","","","/Users/eragon/Zotero/storage/H2QC8KUK/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TRSEHQ2L","preprint","2018","Leino, Klas; Sen, Shayak; Datta, Anupam; Fredrikson, Matt; Li, Linyi","Influence-Directed Explanations for Deep Convolutional Networks","","","","10.48550/arXiv.1802.03788","http://arxiv.org/abs/1802.03788","We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the ""essence"" of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.","2018-11-13","2022-11-28 10:55:46","2022-11-28 10:55:46","2022-11-28 10:55:46","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1802.03788 [cs, stat]","","/Users/eragon/Zotero/storage/KSZYTVZ2/Leino et al. - 2018 - Influence-Directed Explanations for Deep Convoluti.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1802.03788","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NHLL5HNL","preprint","2014","Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew","Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","","","","10.48550/arXiv.1312.6034","http://arxiv.org/abs/1312.6034","This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].","2014-04-19","2022-11-28 10:55:49","2022-11-28 10:55:49","2022-11-28 10:55:49","","","","","","","Deep Inside Convolutional Networks","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1312.6034 [cs]","","/Users/eragon/Zotero/storage/PXVDTL4R/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1312.6034","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"G4V7RT5E","preprint","2013","Zeiler, Matthew D.; Fergus, Rob","Visualizing and Understanding Convolutional Networks","","","","10.48550/arXiv.1311.2901","http://arxiv.org/abs/1311.2901","Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.","2013-11-28","2022-11-28 10:55:55","2022-11-28 10:55:55","2022-11-28 10:55:55","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1311.2901 [cs]","","/Users/eragon/Zotero/storage/TCYYQIJB/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf; /Users/eragon/Zotero/storage/G7Z6NFNF/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1311.2901","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6CCR6JL7","preprint","2019","Yeh, Chih-Kuan; Hsieh, Cheng-Yu; Suggala, Arun Sai; Inouye, David I.; Ravikumar, Pradeep","On the (In)fidelity and Sensitivity for Explanations","","","","10.48550/arXiv.1901.09392","http://arxiv.org/abs/1901.09392","We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.","2019-11-03","2022-11-28 10:56:20","2022-11-28 10:56:20","2022-11-28 10:56:20","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1901.09392 [cs, stat]","","/Users/eragon/Zotero/storage/PEVB52S7/Yeh et al. - 2019 - On the (In)fidelity and Sensitivity for Explanatio.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1901.09392","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WA645RFG","preprint","2018","Ancona, Marco; Ceolini, Enea; Öztireli, Cengiz; Gross, Markus","Towards better understanding of gradient-based attribution methods for Deep Neural Networks","","","","10.48550/arXiv.1711.06104","http://arxiv.org/abs/1711.06104","Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.","2018-03-07","2022-12-06 11:05:01","2022-12-06 11:05:01","2022-12-06 11:05:01","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1711.06104 [cs, stat]","","/Users/eragon/Zotero/storage/N82WWPLB/Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf","","","Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1711.06104","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FD54X7H2","preprint","2013","Zeiler, Matthew D.; Fergus, Rob","Visualizing and Understanding Convolutional Networks","","","","10.48550/arXiv.1311.2901","http://arxiv.org/abs/1311.2901","Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.","2013-11-28","2022-12-07 21:21:56","2022-12-07 21:21:56","2022-12-07 21:21:56","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1311.2901 [cs]","","/Users/eragon/Zotero/storage/IWCBV29B/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1311.2901","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2248BV6C","journalArticle","2021","Zhang, Yu; Tiňo, Peter; Leonardis, Aleš; Tang, Ke","A Survey on Neural Network Interpretability","IEEE Transactions on Emerging Topics in Computational Intelligence","","2471-285X","10.1109/TETCI.2021.3100641","http://arxiv.org/abs/2012.14261","Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.","2021-10","2022-12-07 21:22:25","2022-12-07 21:22:29","2022-12-07 21:22:25","726-742","","5","5","","IEEE Trans. Emerg. Top. Comput. Intell.","","","","","","","","","","","","","arXiv.org","","arXiv:2012.14261 [cs]","","/Users/eragon/Zotero/storage/AF42PZ4M/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JJL8XD4X","conferencePaper","2020","Cubuk, Ekin D.; Zoph, Barret; Shlens, Jonathon; Le, Quoc V.","Randaugment: Practical automated data augmentation with a reduced search space","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","978-1-72819-360-1","","10.1109/CVPRW50498.2020.00359","https://ieeexplore.ieee.org/document/9150790/","Recent work on automated augmentation strategies has led to state-of-the-art results in image classiﬁcation and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We ﬁnd that while previous work required a search for both magnitude and probability of each operation independently, it is sufﬁcient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simpliﬁed search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.","2020-06","2023-01-16 17:03:17","2023-01-16 17:03:17","2023-01-16 17:03:17","3008-3017","","","","","","Randaugment","","","","","IEEE","Seattle, WA, USA","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/F9U8Q2AG/Cubuk et al. - 2020 - Randaugment Practical automated data augmentation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","","","","","","","","","","","","","",""
"CQ5N3UV2","preprint","2022","Steiner, Andreas; Kolesnikov, Alexander; Zhai, Xiaohua; Wightman, Ross; Uszkoreit, Jakob; Beyer, Lucas","How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers","","","","","http://arxiv.org/abs/2106.10270","Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classiﬁcation, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (“AugReg” for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget.1 As one result of this study we ﬁnd that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.","2022-06-23","2023-01-16 17:03:50","2023-01-16 17:03:50","2023-01-16 17:03:50","","","","","","","How to train your ViT?","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2106.10270 [cs]","","/Users/eragon/Zotero/storage/U3YS3MH8/Steiner et al. - 2022 - How to train your ViT Data, Augmentation, and Reg.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2106.10270","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BDFW2A7F","conferencePaper","2020","Jaipuria, Nikita; Zhang, Xianling; Bhasin, Rohan; Arafa, Mayar; Chakravarty, Punarjay; Shrivastava, Shubham; Manglani, Sagar; Murali, Vidya N.","Deflating Dataset Bias Using Synthetic Data Augmentation","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","978-1-72819-360-1","","10.1109/CVPRW50498.2020.00394","https://ieeexplore.ieee.org/document/9150612/","Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-speciﬁc environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the beneﬁts of gaming engine simulations and sim2real style transfer techniques - for ﬁlling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a signiﬁcant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.","2020-06","2023-01-16 17:51:29","2023-01-16 17:51:30","2023-01-16 17:51:29","3344-3353","","","","","","","","","","","IEEE","Seattle, WA, USA","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/EXIZZSHN/Jaipuria et al. - 2020 - Deflating Dataset Bias Using Synthetic Data Augmen.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","","","","","","","","","","","","","",""
"VN8DE33X","journalArticle","","Rebufﬁ, Sylvestre-Alvise; Gowal, Sven; Calian, Dan; Stimberg, Florian; Wiles, Olivia; Mann, Timothy","Data Augmentation Can Improve Robustness","","","","","","Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overﬁtting by using common data augmentation schemes. We demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Furthermore, we compare various data augmentations techniques and observe that spatial composition techniques work best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +2.93% and +2.16% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 60.07% robust accuracy without using any external data. We also achieve a signiﬁcant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TINYIMAGENET.","","2023-01-16 17:54:31","2023-01-16 17:54:33","","","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/eragon/Zotero/storage/DZUY663P/Rebufﬁ et al. - Data Augmentation Can Improve Robustness.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"H9VIVKK6","preprint","2021","Rebuffi, Sylvestre-Alvise; Gowal, Sven; Calian, Dan A.; Stimberg, Florian; Wiles, Olivia; Mann, Timothy","Fixing Data Augmentation to Improve Adversarial Robustness","","","","","http://arxiv.org/abs/2103.01946","Adversarial training suffers from robust overﬁtting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overﬁtting. First, we demonstrate that, contrary to previous ﬁndings, when combined with model weight averaging, data augmentation can signiﬁcantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artiﬁcially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data. Since its original publication (2 Mar 2021), this paper has been accepted to NeurIPS 2021 as two separate and updated papers (Rebufﬁ et al., 2021; Gowal et al., 2021). The new papers improve results and clarity.","2021-10-18","2023-01-16 17:54:52","2023-01-16 17:54:56","2023-01-16 17:54:52","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2103.01946 [cs]","","/Users/eragon/Zotero/storage/H2LK6PX4/Rebuffi et al. - 2021 - Fixing Data Augmentation to Improve Adversarial Ro.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2103.01946","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CSD4FQDJ","preprint","2020","Hendrycks, Dan; Mu, Norman; Cubuk, Ekin D.; Zoph, Barret; Gilmer, Justin; Lakshminarayanan, Balaji","AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty","","","","","http://arxiv.org/abs/1912.02781","Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classiﬁers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX signiﬁcantly improves robustness and uncertainty measures on challenging image classiﬁcation benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.","2020-02-17","2023-01-16 17:56:19","2023-01-16 17:56:19","2023-01-16 17:56:19","","","","","","","AugMix","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1912.02781 [cs, stat]","","/Users/eragon/Zotero/storage/W3F6Q299/Hendrycks et al. - 2020 - AugMix A Simple Data Processing Method to Improve.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:1912.02781","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IU6QMBNL","journalArticle","2020","the Precise4Q consortium; Amann, Julia; Blasimme, Alessandro; Vayena, Effy; Frey, Dietmar; Madai, Vince I.","Explainability for artificial intelligence in healthcare: a multidisciplinary perspective","BMC Medical Informatics and Decision Making","","1472-6947","10.1186/s12911-020-01332-6","https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6","Abstract                            Background               Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.                                         Methods               Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI.                                         Results               Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health.                                         Conclusions               To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.","2020-12","2023-01-17 18:34:40","2023-01-17 18:34:40","2023-01-17 18:34:40","310","","1","20","","BMC Med Inform Decis Mak","Explainability for artificial intelligence in healthcare","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/867RVLHC/the Precise4Q consortium et al. - 2020 - Explainability for artificial intelligence in heal.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SGWD5K4Z","preprint","2018","Yu, Han; Shen, Zhiqi; Miao, Chunyan; Leung, Cyril; Lesser, Victor R.; Yang, Qiang","Building Ethics into Artificial Intelligence","","","","10.48550/arXiv.1812.02953","http://arxiv.org/abs/1812.02953","As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.","2018-12-07","2023-01-17 18:36:59","2023-01-17 18:36:59","2023-01-17 18:36:59","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1812.02953 [cs]","","/Users/eragon/Zotero/storage/XXVPFSPV/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf","","","Computer Science - Artificial Intelligence","","","","","","","","","","","","","","","","","","","arXiv:1812.02953","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MWL4J46R","preprint","2020","Das, Arun; Rad, Paul","Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey","","","","10.48550/arXiv.2006.11371","http://arxiv.org/abs/2006.11371","Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","2020-06-22","2023-01-17 18:37:28","2023-01-17 18:37:28","2023-01-17 18:37:28","","","","","","","Opportunities and Challenges in Explainable Artificial Intelligence (XAI)","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2006.11371 [cs]","","/Users/eragon/Zotero/storage/24NF7HV4/Das and Rad - 2020 - Opportunities and Challenges in Explainable Artifi.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2006.11371","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U2FFMRAC","journalArticle","2022","Ayling, Jacqui; Chapman, Adriane","Putting AI ethics to work: are the tools fit for purpose?","AI and Ethics","","2730-5961","10.1007/s43681-021-00084-x","https://doi.org/10.1007/s43681-021-00084-x","Bias, unfairness and lack of transparency and accountability in Artificial Intelligence (AI) systems, and the potential for the misuse of predictive models for decision-making have raised concerns about the ethical impact and unintended consequences of new technologies for society across every sector where data-driven innovation is taking place. This paper reviews the landscape of suggested ethical frameworks with a focus on those which go beyond high-level statements of principles and offer practical tools for application of these principles in the production and deployment of systems. This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology. We review other historical uses of risk assessments and audits and create a typology that allows us to compare current AI ethics tools to Best Practices found in previous methodologies from technology, environment, privacy, finance and engineering. We analyse current AI ethics tools and their support for diverse stakeholders and components of the AI development and deployment lifecycle as well as the types of tools used to facilitate use. From this, we identify gaps in current AI ethics tools in auditing and risk assessment that should be considered going forward.","2022-08-01","2023-01-17 18:37:47","2023-01-17 18:37:47","2023-01-17 18:37:47","405-429","","3","2","","AI Ethics","Putting AI ethics to work","","","","","","","en","","","","","Springer Link","","","","/Users/eragon/Zotero/storage/3C6HKMTW/Ayling and Chapman - 2022 - Putting AI ethics to work are the tools fit for p.pdf","","","AI; Audit; Ethics; Impact assessment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFAMVYG6","journalArticle","2020","Morley, Jessica; Machado, Caio C.V.; Burr, Christopher; Cowls, Josh; Joshi, Indra; Taddeo, Mariarosaria; Floridi, Luciano","The ethics of AI in health care: A mapping review","Social Science & Medicine","","02779536","10.1016/j.socscimed.2020.113172","https://linkinghub.elsevier.com/retrieve/pii/S0277953620303919","This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be ‘ethically mindful? A series of screening stages were carried out—for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)—yielding a total of 156 papers that were included in the review.","2020-09","2023-01-17 18:38:17","2023-01-17 18:38:18","2023-01-17 18:38:17","113172","","","260","","Social Science & Medicine","The ethics of AI in health care","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/KKP8B7T8/Morley et al. - 2020 - The ethics of AI in health care A mapping review.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MHG4WU8V","journalArticle","2019","Mittelstadt, Brent","Principles alone cannot guarantee ethical AI","Nature Machine Intelligence","","2522-5839","10.1038/s42256-019-0114-4","https://www.nature.com/articles/s42256-019-0114-4","","2019-11-04","2023-01-17 18:38:24","2023-01-17 18:38:24","2023-01-17 18:38:24","501-507","","11","1","","Nat Mach Intell","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/GGNRTJLZ/Mittelstadt - 2019 - Principles alone cannot guarantee ethical AI.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3CG2A5IR","conferencePaper","2020","Zhou, Jianlong; Chen, Fang; Berry, Adam; Reed, Mike; Zhang, Shujia; Savage, Siobhan","A Survey on Ethical Principles of AI and Implementations","2020 IEEE Symposium Series on Computational Intelligence (SSCI)","978-1-72812-547-3","","10.1109/SSCI47803.2020.9308437","https://ieeexplore.ieee.org/document/9308437/","AI has powerful capabilities in prediction, automation, planning, targeting, and personalisation. Generally, it is assumed that AI can enable machines to exhibit human-like intelligence, and is claimed to benefit to different areas of our lives. Since AI is fueled by data and is a distinct form of autonomous and self-learning agency, we are seeing increasing ethical concerns related to AI uses. In order to mitigate various ethical concerns, national and international organisations including governmental organisations, private sectors as well as research institutes have made extensive efforts by drafting ethical principles of AI, and having active discussions on ethics of AI within and beyond the AI community. This paper investigates these efforts with a focus on the identification of fundamental ethical principles of AI and their implementations. The review found that there is a convergence around limited principles and the most prevalent principles are transparency, justice and fairness, responsibility, non-maleficence, and privacy. The investigation suggests that ethical principles need to be combined with every stages of the AI lifecycle in the implementation to ensure that the AI system is designed, implemented and deployed in an ethical manner. Similar to ethical framework used in biomedical and clinical research, this paper suggests checklist-style questionnaires as benchmarks for the implementation of ethical principles of AI.","2020-12-01","2023-01-17 18:38:51","2023-01-17 18:38:51","2023-01-17 18:38:51","3010-3017","","","","","","","","","","","IEEE","Canberra, ACT, Australia","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/T2RRCCHF/Zhou et al. - 2020 - A Survey on Ethical Principles of AI and Implement.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2020 IEEE Symposium Series on Computational Intelligence (SSCI)","","","","","","","","","","","","","","",""
"CGJ6BXKP","webpage","","","Ethical Principles and Governance Technology Development of AI in China | Elsevier Enhanced Reader","","","","","https://reader.elsevier.com/reader/sd/pii/S2095809920300011?token=BCF6223A379F2D2FFF8ED1953A89610B058E50DB52EACA96871713C72D2CF4A6B61BE11552799F45A55E826D0A6575EF&originRegion=eu-west-1&originCreation=20230117183919","","","2023-01-17 18:39:23","2023-01-17 18:39:23","2023-01-17 18:39:23","","","","","","","","","","","","","","en","","","","","","","DOI: 10.1016/j.eng.2019.12.015","","/Users/eragon/Zotero/storage/DKV768WF/Ethical Principles and Governance Technology Devel.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TLPPPVNX","conferencePaper","2022","Birhane, Abeba; Ruane, Elayne; Laurent, Thomas; S. Brown, Matthew; Flowers, Johnathan; Ventresque, Anthony; L. Dancy, Christopher","The Forgotten Margins of AI Ethics","2022 ACM Conference on Fairness, Accountability, and Transparency","978-1-4503-9352-2","","10.1145/3531146.3533157","https://dl.acm.org/doi/10.1145/3531146.3533157","","2022-06-21","2023-01-17 18:39:55","2023-01-17 18:39:55","2023-01-17 18:39:55","948-958","","","","","","","","","","","ACM","Seoul Republic of Korea","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/BMVP5KFZ/Birhane et al. - 2022 - The Forgotten Margins of AI Ethics.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","FAccT '22: 2022 ACM Conference on Fairness, Accountability, and Transparency","","","","","","","","","","","","","","",""
"5B6SJKSN","journalArticle","2021","Lillywhite, Aspen; Wolbring, Gregor","Coverage of ethics within the artificial intelligence and machine learning academic literature: The case of disabled people","Assistive Technology","","1040-0435, 1949-3614","10.1080/10400435.2019.1593259","https://www.tandfonline.com/doi/full/10.1080/10400435.2019.1593259","Disabled people are often the anticipated users of scientific and technological products and processes advanced and enabled by artificial intelligence (AI) and machine learning (ML). Disabled people are also impacted by societal impacts of AI/ML. Many ethical issues are identified within AI/ML as fields and within individual applications of AI/ML. At the same time, problems have been identified in how ethics discourses engage with disabled people. The aim of our scoping review was to better understand to what extent and how the AI/ML focused academic literature engaged with the ethics of AI/ML in relation to disabled people. Of the n = 1659 abstracts engaging with AI/ML and ethics downloaded from Scopus (which includes all Medline articles) and the 70 databases of EBSCO ALL, we found 54 relevant abstracts using the term “patient” and 11 relevant abstracts mentioning terms linked to “impair*”, “disab*” and “deaf”. Our study suggests a gap in the literature that should be filled given the many AI/ML related ethical issues identified in the literature and their impact on disabled people.","2021-05-04","2023-01-17 18:40:13","2023-01-17 18:40:15","2023-01-17 18:40:13","129-135","","3","33","","Assistive Technology","Coverage of ethics within the artificial intelligence and machine learning academic literature","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/JVXBDRJB/Lillywhite and Wolbring - 2021 - Coverage of ethics within the artificial intellige.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K2Y4V7S6","conferencePaper","2011","Torralba, Antonio; Efros, Alexei A.","Unbiased look at dataset bias","CVPR 2011","978-1-4577-0394-2","","10.1109/CVPR.2011.5995347","http://ieeexplore.ieee.org/document/5995347/","","2011-06","2023-01-18 15:19:46","2023-01-18 15:19:46","2023-01-18 15:19:46","1521-1528","","","","","","","","","","","IEEE","Colorado Springs, CO, USA","","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/V9N3FX2F/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"DDBII4F9","preprint","2023","Gozalo-Brizuela, Roberto; Garrido-Merchan, Eduardo C.","ChatGPT is not all you need. A State of the Art Review of large Generative AI models","","","","","http://arxiv.org/abs/2301.04655","During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diﬀusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming eﬀectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientiﬁc texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are aﬀected by generative AI and to provide a taxonomy of the main generative models published recently.","2023-01-11","2023-01-23 11:21:48","2023-01-23 11:21:48","2023-01-23 11:21:48","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:2301.04655 [cs]","","/Users/eragon/Zotero/storage/EV3Q5LBP/Gozalo-Brizuela and Garrido-Merchan - 2023 - ChatGPT is not all you need. A State of the Art Re.pdf","","","Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2301.04655","","","","","","","","","","","","","","","","","","","","","","","","","","",""