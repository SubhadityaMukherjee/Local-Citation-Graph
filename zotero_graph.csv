"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"BTJI7RS7","journalArticle","","Eppel, Sagi","Classifying a specific image region using convolutional nets with an ROI mask as input","","","","","","Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.","","2022-10-03 11:07:30","2022-10-05 13:22:28","","8","","","","","","","","","","","","","en","","","","","Zotero","","","","","","done","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B5WXA4J4","report","2019","Mitsuhara, Masahiro; Fukui, Hiroshi; Sakashita, Yusuke; Ogata, Takanori; Hirakawa, Tsubasa; Yamashita, Takayoshi; Fujiyoshi, Hironobu","Embedding Human Knowledge into Deep Neural Network via Attention Map","","","","","http://arxiv.org/abs/1905.03540","In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.","2019-12-19","2022-10-03 11:09:52","2022-10-05 13:23:02","2022-10-03 11:09:52","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1905.03540 [cs] type: article","","/Users/eragon/Zotero/storage/2SVPKSTS/Mitsuhara et al. - 2019 - Embedding Human Knowledge into Deep Neural Network.pdf","","done","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1905.03540","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"C4YGJLMC","report","2021","Lan, Hai; Wang, Xihao; Wei, Xian","Couplformer:Rethinking Vision Transformer with Coupling Attention Map","","","","","http://arxiv.org/abs/2112.05425","With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.","2021-12-10","2022-10-03 11:11:57","2022-10-03 11:11:57","2022-10-03 11:11:57","","","","","","","Couplformer","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2112.05425 [cs] type: article","","/Users/eragon/Zotero/storage/FB5UAR58/Lan et al. - 2021 - CouplformerRethinking Vision Transformer with Cou.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2112.05425","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"95UVND4X","conferencePaper","2019","Bello, Irwan; Zoph, Barret; Le, Quoc; Vaswani, Ashish; Shlens, Jonathon","Attention Augmented Convolutional Networks","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","978-1-72814-803-8","","10.1109/ICCV.2019.00338","https://ieeexplore.ieee.org/document/9010285/","Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a signiﬁcant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classiﬁcation. We ﬁnd in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classiﬁcation on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3% top-1 accuracy improvement on ImageNet classiﬁcation over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeezeand-Excitation [17]. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.","2019-10","2022-10-03 11:12:55","2022-10-03 11:12:55","2022-10-03 11:12:55","3285-3294","","","","","","","","","","","IEEE","Seoul, Korea (South)","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/WENTJ9EM/Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","","","","","","","","","","","","","","",""
"PAJAZDIQ","journalArticle","2018","Oyama, Taiki; Yamanaka, Takao","Influence of image classification accuracy on saliency map estimation","CAAI Transactions on Intelligence Technology","","2468-2322","10.1049/trit.2018.1012","https://onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1012","Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.","2018","2022-10-03 11:13:35","2022-10-03 11:13:35","2022-10-03 11:13:35","140-152","","3","3","","","","","","","","","","en","","","","","Wiley Online Library","","_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2018.1012","","/Users/eragon/Zotero/storage/HHL5L9G9/Oyama and Yamanaka - 2018 - Influence of image classification accuracy on sali.pdf","","","computer vision; (B6135) Optical; (C5260B) Computer vision and image processing techniques; image and video signal processing; image classification; ImageNet; MIT1003; multiscale images; OSIE; PASCAL-S; saliency-map resolution; up-sampling layer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XCLGFDCD","journalArticle","","Selvaraju, Ramprasaath R; Cogswell, Michael; Das, Abhishek; Vedantam, Ramakrishna; Parikh, Devi; Batra, Dhruv","Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization","","","","","","We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.","","2022-10-03 11:14:00","2022-10-03 11:14:00","","9","","","","","","","","","","","","","en","","","","","Zotero","","","","/Users/eragon/Zotero/storage/EDVF5SE9/Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Z9NJ968D","conferencePaper","2022","Chakraborty, Tanmay; Trehan, Utkarsh; Mallat, Khawla; Dugelay, Jean-Luc","Generalizing Adversarial Explanations with Grad-CAM","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","978-1-66548-739-9","","10.1109/CVPRW56347.2022.00031","https://ieeexplore.ieee.org/document/9857321/","Gradient-weighted Class Activation Mapping (GradCAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.","2022-06","2022-10-03 11:22:46","2022-10-05 13:20:50","2022-10-03 11:22:46","186-192","","","","","","","","","","","IEEE","New Orleans, LA, USA","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/6G98B28D/Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf","","read","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","","","","","","","","","","","","","","",""
"JK3BIH5K","conferencePaper","2018","Chattopadhyay, Aditya; Sarkar, Anirban; Howlader, Prantik; Balasubramanian, Vineeth N.","Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks","2018 IEEE Winter Conference on Applications of Computer Vision (WACV)","","","10.1109/WACV.2018.00097","http://arxiv.org/abs/1710.11063","Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as ""black box"" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.","2018-03","2022-10-03 11:23:49","2022-10-03 11:23:49","2022-10-03 11:23:49","839-847","","","","","","Grad-CAM++","","","","","","","","","","","","arXiv.org","","arXiv:1710.11063 [cs]","","/Users/eragon/Zotero/storage/V2RWR75J/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep .pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZP33PYC6","bookSection","2021","Nunnari, Fabrizio; Kadir, Md Abdul; Sonntag, Daniel","On the Overlap Between Grad-CAM Saliency Maps and Explainable Visual Features in Skin Cancer Images","Machine Learning and Knowledge Extraction","978-3-030-84059-4 978-3-030-84060-0","","","https://link.springer.com/10.1007/978-3-030-84060-0_16","Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identiﬁed on CNNs trained for classiﬁcation. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies signiﬁcantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classiﬁcation correctness shows that higher resolution saliency maps could help doctors in spotting wrong classiﬁcations.","2021","2022-10-03 11:26:23","2022-10-03 11:26:23","2022-10-03 11:26:23","241-253","","","12844","","","","","","","","Springer International Publishing","Cham","en","","","","","DOI.org (Crossref)","","Series Title: Lecture Notes in Computer Science DOI: 10.1007/978-3-030-84060-0_16","","/Users/eragon/Zotero/storage/33H2VKZ5/Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf","","","","Holzinger, Andreas; Kieseberg, Peter; Tjoa, A Min; Weippl, Edgar","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WUSIJ6JR","conferencePaper","2015","Oquab, Maxime; Bottou, Leon; Laptev, Ivan; Sivic, Josef","Is object localization for free? - Weakly-supervised learning with convolutional neural networks","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","978-1-4673-6964-0","","10.1109/CVPR.2015.7298668","http://ieeexplore.ieee.org/document/7298668/","Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classiﬁcation that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classiﬁcation and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We ﬁnd that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.","2015-06","2022-10-21 14:02:58","2022-10-21 14:02:58","2022-10-21 14:02:58","685-694","","","","","","Is object localization for free?","","","","","IEEE","Boston, MA, USA","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/G38P7ZRA/Oquab et al. - 2015 - Is object localization for free - Weakly-supervis.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","","","","","","","","","","","","","","",""
"PYI8S4MY","preprint","2014","Lin, Min; Chen, Qiang; Yan, Shuicheng","Network In Network","","","","","http://arxiv.org/abs/1312.4400","We propose a novel deep network structure called ""Network In Network"" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.","2014-03-04","2022-10-21 14:06:12","2022-10-21 14:06:12","2022-10-21 14:06:12","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1312.4400 [cs] version: 3","","/Users/eragon/Zotero/storage/DNGUDZR4/Lin et al. - 2014 - Network In Network.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Computer Science - Neural and Evolutionary Computing","","","","","","","","","","","","","","","","","","","arXiv:1312.4400","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B7BP5DH8","journalArticle","2020","Zhong, Zhun; Zheng, Liang; Kang, Guoliang; Li, Shaozi; Yang, Yi","Random Erasing Data Augmentation","Proceedings of the AAAI Conference on Artificial Intelligence","","2374-3468, 2159-5399","10.1609/aaai.v34i07.7000","https://aaai.org/ojs/index.php/AAAI/article/view/7000","In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-ﬁtting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and ﬂipping, and yields consistent improvement over strong baselines in image classiﬁcation, object detection and person re-identiﬁcation. Code is available at: https://github.com/zhunzhong07/Random-Erasing.","2020-04-03","2022-10-21 14:13:38","2022-10-21 14:13:38","2022-10-21 14:13:38","13001-13008","","07","34","","AAAI","","","","","","","","en","","","","","DOI.org (Crossref)","","","","/Users/eragon/Zotero/storage/H2HNNUFX/Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EXGB6DN7","conferencePaper","2018","Dvornik, Nikita; Mairal, Julien; Schmid, Cordelia","Modeling Visual Context is Key to Augmenting Object Detection Datasets","","","","","https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html","","2018","2022-10-21 14:20:30","2022-10-21 14:20:30","2022-10-21 14:20:30","364-380","","","","","","","","","","","","","","","","","","openaccess.thecvf.com","","","","/Users/eragon/Zotero/storage/FR4JCRWJ/Dvornik et al. - 2018 - Modeling Visual Context is Key to Augmenting Objec.pdf","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Proceedings of the European Conference on Computer Vision (ECCV)","","","","","","","","","","","","","","",""
"Q6VHCBW3","preprint","2019","Dvornik, Nikita; Mairal, Julien; Schmid, Cordelia","On the Importance of Visual Context for Data Augmentation in Scene Understanding","","","","","http://arxiv.org/abs/1809.02492","Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artiﬁcially increasing the number of training examples, it helps reducing overﬁtting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-speciﬁc prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with signiﬁcant gains in a limited annotation scenario, i.e. when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.","2019-09-19","2022-10-21 14:20:35","2022-10-21 14:20:35","2022-10-21 14:20:35","","","","","","","","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1809.02492 [cs]","","/Users/eragon/Zotero/storage/F4LSAVHY/Dvornik et al. - 2019 - On the Importance of Visual Context for Data Augme.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1809.02492","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"J95RXRLY","preprint","2017","Vaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N.; Kaiser, Lukasz; Polosukhin, Illia","Attention Is All You Need","","","","10.48550/arXiv.1706.03762","http://arxiv.org/abs/1706.03762","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.","2017-12-05","2022-10-21 14:25:17","2022-10-21 14:25:17","2022-10-21 14:25:16","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1706.03762 [cs]","","/Users/eragon/Zotero/storage/WIMZKX6P/Vaswani et al. - 2017 - Attention Is All You Need.pdf","","","Computer Science - Machine Learning; Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1706.03762","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AI9B467H","preprint","2019","Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina","BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding","","","","10.48550/arXiv.1810.04805","http://arxiv.org/abs/1810.04805","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","2019-05-24","2022-10-21 14:35:45","2022-10-21 14:35:45","2022-10-21 14:35:44","","","","","","","BERT","","","","","arXiv","","","","","","","arXiv.org","","arXiv:1810.04805 [cs]","","/Users/eragon/Zotero/storage/UKB798MK/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:1810.04805","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TGHIABCB","preprint","2020","Brown, Tom B.; Mann, Benjamin; Ryder, Nick; Subbiah, Melanie; Kaplan, Jared; Dhariwal, Prafulla; Neelakantan, Arvind; Shyam, Pranav; Sastry, Girish; Askell, Amanda; Agarwal, Sandhini; Herbert-Voss, Ariel; Krueger, Gretchen; Henighan, Tom; Child, Rewon; Ramesh, Aditya; Ziegler, Daniel M.; Wu, Jeffrey; Winter, Clemens; Hesse, Christopher; Chen, Mark; Sigler, Eric; Litwin, Mateusz; Gray, Scott; Chess, Benjamin; Clark, Jack; Berner, Christopher; McCandlish, Sam; Radford, Alec; Sutskever, Ilya; Amodei, Dario","Language Models are Few-Shot Learners","","","","10.48550/arXiv.2005.14165","http://arxiv.org/abs/2005.14165","Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.","2020-07-22","2022-10-21 14:36:26","2022-10-21 14:36:26","2022-10-21 14:36:26","","","","","","","","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2005.14165 [cs]","","/Users/eragon/Zotero/storage/LZZ4A45Z/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf","","","Computer Science - Computation and Language","","","","","","","","","","","","","","","","","","","arXiv:2005.14165","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"U7THJY2A","preprint","2021","Dosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil","An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale","","","","10.48550/arXiv.2010.11929","http://arxiv.org/abs/2010.11929","While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.","2021-06-03","2022-10-21 14:38:41","2022-10-21 14:38:41","2022-10-21 14:38:41","","","","","","","An Image is Worth 16x16 Words","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2010.11929 [cs]","","/Users/eragon/Zotero/storage/2VL6R42V/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Artificial Intelligence; Computer Science - Machine Learning","","","","","","","","","","","","","","","","","","","arXiv:2010.11929","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LYNHJ7BG","preprint","2022","Liu, Ze; Hu, Han; Lin, Yutong; Yao, Zhuliang; Xie, Zhenda; Wei, Yixuan; Ning, Jia; Cao, Yue; Zhang, Zheng; Dong, Li; Wei, Furu; Guo, Baining","Swin Transformer V2: Scaling Up Capacity and Resolution","","","","","http://arxiv.org/abs/2111.09883","Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536$\times$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \url{https://github.com/microsoft/Swin-Transformer}.","2022-04-11","2022-10-21 15:13:27","2022-10-21 15:13:27","2022-10-21 15:13:27","","","","","","","Swin Transformer V2","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2111.09883 [cs] version: 2","","/Users/eragon/Zotero/storage/I53USPS2/Liu et al. - 2022 - Swin Transformer V2 Scaling Up Capacity and Resol.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:2111.09883","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5YFWZN73","preprint","2021","Kostrikov, Ilya; Yarats, Denis; Fergus, Rob","Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels","","","","","http://arxiv.org/abs/2004.13649","We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.","2021-03-07","2022-11-07 21:41:19","2022-11-07 21:41:19","2022-11-07 21:41:19","","","","","","","Image Augmentation Is All You Need","","","","","arXiv","","","","","","","arXiv.org","","arXiv:2004.13649 [cs, eess, stat]","","/Users/eragon/Zotero/storage/FYYYKT7F/Kostrikov et al. - 2021 - Image Augmentation Is All You Need Regularizing D.pdf","","","Computer Science - Computer Vision and Pattern Recognition; Computer Science - Machine Learning; Statistics - Machine Learning; Electrical Engineering and Systems Science - Image and Video Processing","","","","","","","","","","","","","","","","","","","arXiv:2004.13649","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XK4PKS42","preprint","2014","Simonyan, Karen; Vedaldi, Andrea; Zisserman, Andrew","Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps","","","","","http://arxiv.org/abs/1312.6034","This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].","2014-04-19","2022-11-18 12:27:28","2022-11-18 12:27:28","2022-11-18 12:27:28","","","","","","","Deep Inside Convolutional Networks","","","","","arXiv","","en","","","","","arXiv.org","","arXiv:1312.6034 [cs]","","/Users/eragon/Zotero/storage/3TNFZE58/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf","","","Computer Science - Computer Vision and Pattern Recognition","","","","","","","","","","","","","","","","","","","arXiv:1312.6034","","","","","","","","","","","","","","","","","","","","","","","","","","",""